{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saurabh Pradhan\n",
    "## 277643"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The general setup of neural network is as follows :-\n",
    "The input to the neural network is a vector of dimension (m , 1) where m it the total no of features, bias excluded.<br /> The dimensions of bias is (m, 1)\n",
    "From m dimensional layer to other n dimensional layer a weight matrix is attached. The dimension of this weight matrix is (m, n) <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squaredError(y, yPrediction, derivative = False):\n",
    "    if derivative == False:\n",
    "        return np.sum(((y - yPrediction) ** 2))\n",
    "    else:\n",
    "        return -1 * (y - yPrediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(x, derivative = False):\n",
    "    if derivative == False:\n",
    "        return np.maximum(x, 0)\n",
    "    else:\n",
    "        \"\"\"Deep copy\"\"\"\n",
    "        x = x.copy()\n",
    "        x[x<0] = 0\n",
    "        x[x>0] = 1\n",
    "        return x\n",
    "\n",
    "def linearActivation(x, derivative = False):\n",
    "    if derivative == False:\n",
    "        return x\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1Regularization(gradients):\n",
    "    \"\"\"Deep copy\"\"\"\n",
    "    gradients = gradients.copy()\n",
    "    gradients[gradients > 0] = 1\n",
    "    gradients[gradients <= 0] = -1\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class STRUCTURE(Enum):\n",
    "    WEIGHTMATRIX = \"weightMatrix\"\n",
    "    ACTIVATIONFUNCTION = \"activationFunction\"\n",
    "    BIAS = \"bias\"\n",
    "    INPUTLAYER = \"inputLayer\"\n",
    "    REGULARIZATION = \"regularization\"\n",
    "    REGULARIZATIONPENALTY = \"regularizationPenalty\"\n",
    "    \n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    networkStructure = None\n",
    "    lossFunction = None\n",
    "            \n",
    "    def __init__(self, networkStructure, lossFunction):\n",
    "        self.networkStructure = networkStructure\n",
    "        self.lossFunction = lossFunction\n",
    "    \n",
    "    def forwardPropogationStep(self, x, retainIntermediateValues = False):\n",
    "        output = x\n",
    "        intermediateValues = []\n",
    "        for layer in self.networkStructure:\n",
    "                aggregation = np.dot(output, layer[STRUCTURE.WEIGHTMATRIX]) + layer[STRUCTURE.BIAS]\n",
    "                if retainIntermediateValues == True:\n",
    "                    intermediateValues.append([output, aggregation])\n",
    "\n",
    "                output = layer[STRUCTURE.ACTIVATIONFUNCTION](aggregation)\n",
    "        return output, intermediateValues\n",
    "\n",
    "    def backPropogationStep(self, x, y, learningRate):\n",
    "        prediction, intermediateValues = self.forwardPropogationStep(x, True)\n",
    "        loss = self.lossFunction(y, prediction)\n",
    "        print(\"Calculated loss at end of forward propogation \" +str(loss))\n",
    "        backPropogationComponent = self.lossFunction(y, prediction, derivative = True)\n",
    "        for layer, intermediateValue in zip(reversed(self.networkStructure), reversed(intermediateValues)):\n",
    "            \n",
    "            backPropogationComponent = backPropogationComponent * layer[STRUCTURE.ACTIVATIONFUNCTION](intermediateValue[1], \n",
    "                                                                                            derivative = True)\n",
    "            \"\"\"Update weights in current layer\"\"\"\n",
    "            weightGradient = intermediateValue[0].T * backPropogationComponent\n",
    "            biasGradient = 1 * backPropogationComponent\n",
    "            \n",
    "            regularization = 0\n",
    "            if STRUCTURE.REGULARIZATION in layer:\n",
    "                regularization = layer[STRUCTURE.REGULARIZATION](weightGradient)\n",
    "                regularization = regularization * layer[STRUCTURE.REGULARIZATIONPENALTY]\n",
    "            \n",
    "            \"\"\"Dirty update\"\"\"\n",
    "            layer[STRUCTURE.WEIGHTMATRIX] = layer[STRUCTURE.WEIGHTMATRIX] - learningRate * (weightGradient +\n",
    "                                                                                            regularization)\n",
    "            layer[STRUCTURE.BIAS] = layer[STRUCTURE.BIAS] - learningRate * biasGradient\n",
    "                       \n",
    "            \n",
    "            if layer[STRUCTURE.INPUTLAYER] == True:\n",
    "                break\n",
    "            \n",
    "            \"\"\"Update in the previous layer depends on the output of activation function\"\"\"\n",
    "            backPropogationComponent = backPropogationComponent * layer[STRUCTURE.WEIGHTMATRIX].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial output is [-1.]\n",
      "The loss is 9.0\n"
     ]
    }
   ],
   "source": [
    "structure = [\n",
    "    {\n",
    "        STRUCTURE.WEIGHTMATRIX:np.array([-1, 2, 0, -2]).reshape(2, 2),\n",
    "        STRUCTURE.BIAS: np.ones(2).reshape(1, 2),\n",
    "        STRUCTURE.ACTIVATIONFUNCTION : relu,\n",
    "        STRUCTURE.INPUTLAYER:True\n",
    "    },\n",
    "    {\n",
    "        STRUCTURE.WEIGHTMATRIX:np.array([-1, 2]).reshape(2, 1),\n",
    "        STRUCTURE.BIAS: np.ones(1).reshape(1, 1),\n",
    "        STRUCTURE.ACTIVATIONFUNCTION : linearActivation,\n",
    "        STRUCTURE.INPUTLAYER:False\n",
    "    }\n",
    "]\n",
    "\n",
    "inputVector = np.array([-1, 1]).reshape(1, -1) \n",
    "nn = NeuralNetwork(structure, squaredError)\n",
    "output = nn.forwardPropogationStep(inputVector)[0][0]\n",
    "print(\"The initial output is \" + str(output))\n",
    "print(\"The loss is \"+ str(squaredError(2, output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated loss at end of forward propogation 9.0\n",
      "New output is [-0.770476]\n",
      "New loss is 7.675537266576\n"
     ]
    }
   ],
   "source": [
    "nn.backPropogationStep(inputVector, 2, 0.01)\n",
    "output = nn.forwardPropogationStep(inputVector)\n",
    "output = nn.forwardPropogationStep(inputVector)[0][0]\n",
    "print(\"New output is \" + str(output))\n",
    "print(\"New loss is \"+ str(squaredError(2, output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias weight\n",
      "[[0.9718 1.    ]]\n",
      "Weight\n",
      "[[-0.9718  2.    ]\n",
      " [-0.0282 -2.    ]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bias weight\n",
      "[[1.03]]\n",
      "Weight\n",
      "[[-0.94]\n",
      " [ 2.  ]]\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for layer in nn.networkStructure:\n",
    "    print(\"Bias weight\")\n",
    "    print(layer[STRUCTURE.BIAS])\n",
    "    print(\"Weight\")\n",
    "    print(layer[STRUCTURE.WEIGHTMATRIX])\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial output is [-1.]\n",
      "The loss is 9.0\n"
     ]
    }
   ],
   "source": [
    "structure = [\n",
    "    {\n",
    "        STRUCTURE.WEIGHTMATRIX:np.array([-1, 2, 0, -2]).reshape(2, 2),\n",
    "        STRUCTURE.BIAS: np.ones(2).reshape(1, 2),\n",
    "        STRUCTURE.ACTIVATIONFUNCTION : relu,\n",
    "        STRUCTURE.INPUTLAYER:True,\n",
    "        STRUCTURE.REGULARIZATION:L1Regularization,\n",
    "        STRUCTURE.REGULARIZATIONPENALTY:0.5\n",
    "    },\n",
    "    {\n",
    "        STRUCTURE.WEIGHTMATRIX:np.array([-1, 2]).reshape(2, 1),\n",
    "        STRUCTURE.BIAS: np.ones(1).reshape(1, 1),\n",
    "        STRUCTURE.ACTIVATIONFUNCTION : linearActivation,\n",
    "        STRUCTURE.INPUTLAYER:False,\n",
    "        STRUCTURE.REGULARIZATION:L1Regularization,\n",
    "        STRUCTURE.REGULARIZATIONPENALTY:0.5\n",
    "    }\n",
    "]\n",
    "\n",
    "inputVector = np.array([-1, 1]).reshape(1, -1) \n",
    "nn = NeuralNetwork(structure, squaredError)\n",
    "output = nn.forwardPropogationStep(inputVector)[0][0]\n",
    "print(\"The initial output is \" + str(output))\n",
    "print(\"The loss is \"+ str(squaredError(2, output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated loss at end of forward propogation 9.0\n",
      "New output is [-0.75196975]\n",
      "New loss is 7.573337504915064\n"
     ]
    }
   ],
   "source": [
    "nn.backPropogationStep(inputVector, 2, 0.01)\n",
    "output = nn.forwardPropogationStep(inputVector)\n",
    "output = nn.forwardPropogationStep(inputVector)[0][0]\n",
    "print(\"New output is \" + str(output))\n",
    "print(\"New loss is \"+ str(squaredError(2, output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias weight\n",
      "[[0.97195 1.     ]]\n",
      "Weight\n",
      "[[-0.96695  2.005  ]\n",
      " [-0.03305 -1.995  ]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bias weight\n",
      "[[1.03]]\n",
      "Weight\n",
      "[[-0.935]\n",
      " [ 2.005]]\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for layer in nn.networkStructure:\n",
    "    print(\"Bias weight\")\n",
    "    print(layer[STRUCTURE.BIAS])\n",
    "    print(\"Weight\")\n",
    "    print(layer[STRUCTURE.WEIGHTMATRIX])\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After regularization the loss has reduced a little, however the difference is very less (0.01)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation allows to create additional data/ synthetic data from available training data. One way is to add white noise to the audio. The amplitude of white noise should be kept low so that the words are clearly audiable. Second method is to shuffle the sound and create new data points. Other methods include streching sounds and changing the pitch of the audio clip. Also the dynamic range of autio can also be changed to generate samples.\n",
    "<ul>\n",
    "    <li>Time streching</li>\n",
    "    <li>Pitch Shifting</li>\n",
    "    <li>Dynamic range compression</li>\n",
    "    <li>Background noise</li>\n",
    "</ul>\n",
    "These are the methods that can be used for generating synthetic data for sound dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation helps with regularization. This is because the augmented data adds prior knowledge for the model and reduces model varience interm reducing the complexicity of the model. Hence data augmentation can be used as regularization method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
